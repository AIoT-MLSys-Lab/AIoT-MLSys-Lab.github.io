"use strict";(self.webpackChunklabwebsite=self.webpackChunklabwebsite||[]).push([[776],{4243:function(e,i,t){t.d(i,{Z:function(){return s}});t(2791);var a=t(184),s=function(e){var i=e.link,t=e.imageURL,s=e.title,n=e.content;return(0,a.jsx)("div",{children:(0,a.jsx)("a",{href:i,className:"projectContainer",children:(0,a.jsxs)("div",{className:"projectPanel",children:[(0,a.jsx)("div",{className:"projectPanelTitle",children:s}),(0,a.jsxs)("div",{className:"projectFlex",children:[(0,a.jsx)("div",{className:"projectPanelImgContainer",children:(0,a.jsx)("img",{src:t,alt:"",className:"projectPanelImg"})}),(0,a.jsx)("div",{className:"projectPanelTextContainer",children:(0,a.jsx)("div",{className:"projectPanelContent",children:(0,a.jsx)("div",{dangerouslySetInnerHTML:{__html:n}})})})]})]})})})}},7776:function(e,i,t){t.r(i);t(2791);var a=t(4676),s=(t(6822),t(4243)),n=t(184);i.default=function(){return(0,n.jsxs)("div",{children:[(0,n.jsx)(a.Z,{title:"Edge AI",subtitle:"Research"}),(0,n.jsx)(s.Z,{link:"./#/EdgeAI/AIoTSurvey",imageURL:"./images/projects/AIoT_overview.svg",title:"Artificial Intelligence of Things: A Survey",content:"The integration of the Internet of Things (IoT) and modern Artificial Intelligence (AI) has given rise to a new paradigm known as the Artificial Intelligence of Things (AIoT). In this survey, we provide a systematic and comprehensive review of AIoT research. We examine AIoT literature related to sensing, computing, and networking & communication, which form the three key components of AIoT. In addition to advancements in these areas, we review domain-specific AIoT systems that are designed for various important application domains. We have also created an accompanying GitHub repository, where we compile the papers included in this survey: https://github.com/AIoT-MLSys-Lab/AIoT-Survey. This repository will be actively maintained and updated with new research as it becomes available. As both IoT and AI become increasingly critical to our society, we believe AIoT is emerging as an essential research field at the intersection of IoT and modern AI. We hope this survey will serve as a valuable resource for those engaged in AIoT research and act as a catalyst for future explorations to bridge gaps and drive advancements in this exciting field."}),(0,n.jsx)(s.Z,{link:"./#/EdgeAI/IoTinEraOfGAI",imageURL:"./images/projects/IoTinGAI.svg",title:"IoT in the Era of Generative AI: Vision and Challenges",content:"Equipped with sensing, networking, and computing capabilities, Internet of Things (IoT) such as smartphones, wearables, smart speakers, and household robots have been seamlessly weaved into our daily lives. Recent advancements in Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold immense promise to push IoT to the next level. In this article, we share our vision and views on the benefits that Generative AI brings to IoT, and discuss some of the most important applications of Generative AI in IoT-related domains. Fully harnessing Generative AI in IoT is a complex challenge. We identify some of the most critical challenges including high resource demands of the Generative AI models, prompt engineering, on-device inference, offloading, on-device fine-tuning, federated learning, security, as well as development tools and benchmarks, and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new research on IoT in the era of Generative AI."}),(0,n.jsx)(s.Z,{link:"./#/EdgeAI/NestDNN",imageURL:"./images/projects/NestDNN.svg",title:"NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning for Continuous Mobile Vision",content:"Mobile vision systems such as smartphones, drones, and augmented-reality headsets are revolutionizing our lives. These systems usually run multiple applications concurrently and their available resources at runtime are dynamic due to events such as starting new applications, closing existing applications, and application priority changes. In this work, we present NestDNN, a framework that takes the dynamics of runtime resources into account to enable resource-aware multi-tenant on-device deep learning for mobile vision systems. NestDNN enables each deep learning model to offer flexible resource-accuracy trade-offs. At runtime, it dynamically selects the optimal resource-accuracy trade-off for each deep learning model to fit the model's resource demand to the system's available runtime resources. In doing so, NestDNN efficiently utilizes the limited resources in mobile vision systems to jointly maximize the performance of all the concurrently running applications. Our experiments show that compared to the resource-agnostic status quo approach, NestDNN achieves as much as 4.2% increase in inference accuracy, 2.0\xd7 increase in video frame processing rate and 1.7\xd7 reduction on energy consumption."}),(0,n.jsx)(s.Z,{link:"./#/EdgeAI/FlexDNN",imageURL:"./images/projects/FlexDNN.svg",title:"FlexDNN: Input-Adaptive On-Device Deep Learning for Efficient Mobile Vision",content:"Mobile vision systems powered by the recent advancement in Deep Neural Networks (DNNs) are enabling a wide range of on-device video analytics applications. Considering mobile systems are constrained with limited resources, reducing resource demands of DNNs is crucial to realizing the full potential of these applications. In this work, we present FlexDNN, an input-adaptive DNN-based framework for efficient on-device video analytics. To achieve this, FlexDNN takes the intrinsic dynamics of mobile videos into consideration, and dynamically adapts its model complexity to the difficulty levels of input video frames to achieve computation efficiency. FlexDNN addresses the key drawbacks of existing systems and pushes the state-of-the-art forward. We use FlexDNN to build three representative on-device video analytics applications, and evaluate its performance on both mobile CPU and GPU platforms. Our results show that FlexDNN significantly outperforms status quo approaches in accuracy, average CPU/GPU processing time per frame, frame drop rate, and energy consumption."}),(0,n.jsx)(s.Z,{link:"./#/EdgeAI/Mercury",imageURL:"./images/projects/Mercury.svg",title:"Mercury: Efficient On-Device Distributed DNN Training via Stochastic Importance Sampling",content:"As intelligence is moving from data centers to the edges, intelligent edge devices such as smartphones, drones, robots, and smart IoT devices are equipped with the capability to altogether train a deep learning model on the devices from the data collected by themselves. Despite its considerable value, the key bottleneck of making on-device distributed training practically useful in real-world deployments is that they consume a significant amount of training time under wireless networks with constrained bandwidth. To tackle this critical bottleneck, we present Mercury, an importance sampling-based framework that enhances the training efficiency of on-device distributed training without compromising the accuracies of the trained models. The key idea behind the design of Mercury is to focus on samples that provide more important information in each training iteration. In doing this, the training efficiency of each iteration is improved. As such, the total number of iterations can be considerably reduced so as to speed up the overall training process. We implemented Mercury and deployed it on a self-developed testbed. We demonstrate its effectiveness and show that Mercury consistently outperforms two status quo frameworks on six commonly used datasets across tasks in image classification, speech recognition, and natural language processing."}),(0,n.jsx)(s.Z,{link:"./#/EdgeAI/Distream",imageURL:"./images/projects/Distream.svg",title:"Distream: Scaling Live Video Analytics with Workload-Adaptive Distributed Edge Intelligence",content:"Video cameras have been deployed at scale today. Driven by the breakthrough in deep learning (DL), organizations that have deployed these cameras start to use DL-based techniques for live video analytics. Although existing systems aim to optimize live video analytics from a variety of perspectives, they are agnostic to the workload dynamics in real-world deployments. In this work, we present Distream, a distributed live video analytics system based on the smart camera-edge cluster architecture, that is able to adapt to the workload dynamics to achieve low-latency, high-throughput, and scalable live video analytics. The key behind the design of Distream is to adaptively balance the workloads across smart cameras and partition the workloads between cameras and the edge cluster. In doing so, Distream is able to fully utilize the compute resources at both ends to achieve optimized system performance. We evaluated Distream with 500 hours of distributed video streams from two real-world video datasets with a testbed that consists of 24 cameras and a 4-GPU edge cluster. Our results show that Distream consistently outperforms the status quo in terms of throughput, latency, and latency service level objective (SLO) miss rate."})]})}},6822:function(){}}]);
//# sourceMappingURL=776.4e8f350c.chunk.js.map