"use strict";(self.webpackChunklabwebsite=self.webpackChunklabwebsite||[]).push([[744],{876:function(e,t,s){var i=s(8316),a=s(184),n={height:"300px",color:"#fff",lineHeight:"160px",textAlign:"center",background:"#364d79"};t.Z=function(){return(0,a.jsxs)(i.Z,{autoplay:!0,children:[(0,a.jsx)("div",{children:(0,a.jsx)("h3",{style:n,children:"1"})}),(0,a.jsx)("div",{children:(0,a.jsx)("h3",{style:n,children:"2"})}),(0,a.jsx)("div",{children:(0,a.jsx)("h3",{style:n,children:"3"})}),(0,a.jsx)("div",{children:(0,a.jsx)("h3",{style:n,children:"4"})})]})}},4996:function(e,t,s){s.d(t,{Z:function(){return o}});var i=s(9439),a=s(2791),n=s(969),r=s(7309),l=s(184),o=function(e){var t=(0,a.useState)(!1),s=(0,i.Z)(t,2),o=s[0],c=s[1],d=function(){c(!1)};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsxs)("button",{href:"",className:"project_button",onClick:function(){c(!0)},children:[(0,l.jsx)("svg",{t:"1687730915571",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"2419",width:"16",height:"16",children:(0,l.jsx)("path",{d:"M313.6 170.666667h469.333333c27.733333 0 42.666667 14.933333 42.666667 42.666666v512c0 8.533333-4.266667 17.066667-10.666667 23.466667l-36.266666 36.266667c-2.133333 2.133333-6.4 4.266667-6.4 2.133333-2.133333 0-2.133333-4.266667-2.133334-6.4v-539.733333c0-4.266667-2.133333-6.4-4.266666-10.666667s-6.4-4.266667-10.666667-4.266667h-398.933333c-8.533333 0-17.066667 4.266667-23.466667 10.666667l-36.266667 36.266667c-2.133333 2.133333-4.266667 6.4-2.133333 6.4 0 2.133333 4.266667 2.133333 6.4 2.133333h398.933333c4.266667 0 6.4 2.133333 10.666667 4.266667s4.266667 6.4 4.266667 10.666666v539.733334c0 4.266667-2.133333 6.4-4.266667 10.666666s-6.4 4.266667-10.666667 4.266667H213.333333c-4.266667 0-6.4-2.133333-10.666666-4.266667s-4.266667-6.4-4.266667-10.666666v-554.666667c0-8.533333 4.266667-17.066667 10.666667-23.466667l78.933333-78.933333c6.4-4.266667 14.933333-8.533333 25.6-8.533333z","p-id":"2420",fill:"#ffffff"})}),"\xa0BibTex"]}),(0,l.jsx)(n.Z,{title:"BibTex",open:o,onCancel:d,width:1e3,footer:[(0,l.jsx)(r.ZP,{onClick:d,children:"Return"},"back")],children:(0,l.jsxs)("div",{className:"popModalBG",children:[(0,l.jsx)("p",{children:e.inproceeding}),(0,l.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.title]}),(0,l.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.author]}),(0,l.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.booktitle]}),e.pages&&(0,l.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.pages]}),(0,l.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.year]}),e.numpages&&(0,l.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.numpages]}),e.address&&(0,l.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.address]}),e.location&&(0,l.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.location]}),(0,l.jsx)("p",{children:"}"})]})})]})}},3744:function(e,t,s){s.r(t),s.d(t,{default:function(){return r}});s(2791);var i=s(876),a=s(4996),n=s(184);var r=function(){return(0,n.jsxs)("div",{className:"project_container",children:[(0,n.jsx)("div",{className:"project_title",children:"SharpEar: Real-Time Speech Enhancement in Noisy Environments"}),(0,n.jsx)("div",{className:"project_conference"}),(0,n.jsx)("div",{className:"project_members",children:"Xiao Zeng, Kai Cao, Haochen Sun, and Mi Zhang."}),(0,n.jsxs)("div",{className:"project_buttons",children:[(0,n.jsxs)("a",{href:"",className:"project_button",children:[(0,n.jsx)("svg",{t:"1687728426228",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"2399",width:"16",height:"16",children:(0,n.jsx)("path",{d:"M213.34016 0l597.34016 0q53.00224 0 90.50112 37.49888t37.49888 90.50112l0 768q0 53.00224-37.49888 90.50112t-90.50112 37.49888l-597.34016 0q-53.00224 0-90.50112-37.49888t-37.49888-90.50112l0-768q0-53.00224 37.49888-90.50112t90.50112-37.49888zM341.34016 725.34016l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM341.34016 554.65984l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM810.65984 85.34016l-597.34016 0q-17.67424 0-30.16704 12.4928t-12.4928 30.16704l0 768q0 17.67424 12.4928 30.16704t30.16704 12.4928l597.34016 0q17.67424 0 30.16704-12.4928t12.4928-30.16704l0-768q0-17.67424-12.4928-30.16704t-30.16704-12.4928zM341.34016 384l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM341.34016 213.34016l170.65984 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-170.65984 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928z",fill:"#ffffff","p-id":"2400"})}),"\xa0Paper"]}),(0,n.jsx)(a.Z,{inproceeding:"@inproceedings{zeng2017mobiledeeppill,",title:"title = {{MobileDeepPill: A Small-Footprint Mobile Deep Learning System for Recognizing Unconstrained Pill Images}},",author:"author = {Zeng, Xiao and Cao, Kai and Zhang, Mi},",booktitle:"booktitle = {Proceedings of the 15th ACM International Conference on Mobile Systems, Applications, and Services (MobiSys)},",pages:"pages = {56--67},",year:"year = {2017},",numpages:"numpages = {12},",address:"address = {Niagara Falls, NY, USA}"})]}),(0,n.jsx)("div",{className:"project_collection",children:(0,n.jsx)(i.Z,{})}),(0,n.jsxs)("div",{className:"project_abstract",children:[(0,n.jsx)("div",{className:"project_abstract_title",children:"Abstract"}),(0,n.jsx)("div",{className:"project_abstract_content",children:"Video cameras have been deployed at scale today. Driven by the breakthrough in deep learning (DL), organizations that have deployed these cameras start to use DL-based techniques for live video analytics. Although existing systems aim to optimize live video analytics from a variety of perspectives, they are agnostic to the workload dynamics in real-world deployments. In this work, we present Distream, a distributed live video analytics system based on the smart camera-edge cluster architecture, that is able to adapt to the workload dynamics to achieve low-latency, high-throughput, and scalable live video analytics. The key behind the design of Distream is to adaptively balance the workloads across smart cameras and partition the workloads between cameras and the edge cluster. In doing so, Distream is able to fully utilize the compute resources at both ends to achieve optimized system performance. We evaluated Distream with 500 hours of distributed video streams from two real-world video datasets with a testbed that consists of 24 cameras and a 4-GPU edge cluster. Our results show that Distream consistently outperforms the status quo in terms of throughput, latency, and latency service level objective (SLO) miss rate."})]})]})}}}]);
//# sourceMappingURL=744.b2e8f10e.chunk.js.map