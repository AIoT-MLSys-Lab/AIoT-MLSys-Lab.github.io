"use strict";(self.webpackChunklabwebsite=self.webpackChunklabwebsite||[]).push([[324],{4243:function(e,i,n){n.d(i,{Z:function(){return a}});n(2791);var t=n(184),a=function(e){var i=e.link,n=e.imageURL,a=e.title,o=e.content;return(0,t.jsx)("div",{children:(0,t.jsx)("a",{href:i,className:"projectContainer",children:(0,t.jsxs)("div",{className:"projectPanel",children:[(0,t.jsx)("div",{className:"projectPanelTitle",children:a}),(0,t.jsxs)("div",{className:"projectFlex",children:[(0,t.jsx)("div",{className:"projectPanelImgContainer",children:(0,t.jsx)("img",{src:n,alt:"",className:"projectPanelImg"})}),(0,t.jsx)("div",{className:"projectPanelTextContainer",children:(0,t.jsx)("div",{className:"projectPanelContent",children:(0,t.jsx)("div",{dangerouslySetInnerHTML:{__html:o}})})})]})]})})})}},6324:function(e,i,n){n.r(i);n(2791);var t=n(4676),a=(n(6822),n(4243)),o=n(184);i.default=function(){return(0,o.jsxs)("div",{children:[(0,o.jsx)(t.Z,{title:"Mobile Health",subtitle:"Research"}),(0,o.jsx)(a.Z,{link:"./#/MobileHealth/MEIT",imageURL:"./images/projects/MEIT.png",title:"MEIT: Multi-Modal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation",content:"Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for monitoring cardiac conditions and is crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is time-consuming and requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the first attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open-source LLMs using more than 800,000 ECG reports. MEIT's results underscore the superior performance of instruction-tuned LLMs, showcasing their proficiency in quality report generation, zero-shot capabilities, and resilience to signal perturbation. These findings emphasize the efficacy of our MEIT framework and its potential for real-world clinical application."}),(0,o.jsx)(a.Z,{link:"./#/MobileHealth/MobileDeepPill",imageURL:"./images/projects/MobileDeepPill.svg",title:"MobileDeepPill: A Small-Footprint Mobile Deep Learning System for Recognizing Unconstrained Pill Images",content:"Correct identification of prescription pills based on their visual appearance is a key step required to assure patient safety and facilitate more effective patient care. With the availability of high-quality cameras and computational power on smartphones, it is possible and helpful to identify unknown prescription pills using smartphones. Towards this goal, in 2016, the U.S. National Library of Medicine (NLM) of the National Institutes of Health (NIH) announced a nationwide competition, calling for the creation of a mobile vision system that can recognize pills automatically from a mobile phone picture under unconstrained real-world settings. In this work, we present the design and evaluation of such mobile pill image recognition system called MobileDeepPill. The development of MobileDeepPill involves three key innovations: a triplet loss function which attains invariances to real-world noisiness that deteriorates the quality of pill images taken by mobile phones; a multi-CNNs model that collectively captures the shape, color and imprints characteristics of the pills; and a Knowledge Distillation-based deep model compression framework that significantly reduces the size of the multi-CNNs model without deteriorating its recognition performance. Our deep learning-based pill image recognition algorithm wins the First Prize (champion) of the NIH NLM Pill Image Recognition Challenge. Given its promising performance, we believe MobileDeepPill helps NIH tackle a critical problem with significant societal impact and will benefit millions of healthcare personnel and the general public."}),(0,o.jsx)(a.Z,{link:"./#/MobileHealth/DeepASL",imageURL:"./images/projects/DeepASL.svg",title:"DeepASL: Enabling Ubiquitous and Non-Intrusive Word and Sentence-Level Sign Language Translation",content:"There is an undeniable communication barrier between deaf people and people with normal hearing ability. Although innovations in sign language translation technology aim to tear down this communication barrier, the majority of existing sign language translation systems are either intrusive or constrained by resolution or ambient lighting conditions. Moreover, these existing systems can only perform single-sign ASL translation rather than sentence-level translation, making them much less useful in daily-life communication scenarios. In this work, we fill this critical gap by presenting DeepASL, a transformative deep learning-based sign language translation technology that enables ubiquitous and non-intrusive American Sign Language (ASL) translation at both word and sentence levels. DeepASL uses infrared light as its sensing mechanism to non-intrusively capture the ASL signs. It incorporates a novel hierarchical bidirectional deep recurrent neural network (HB-RNN) and a probabilistic framework based on Connectionist Temporal Classification (CTC) for word-level and sentence-level ASL translation respectively. To evaluate its performance, we have collected 7, 306 samples from 11 participants, covering 56 commonly used ASL words and 100 ASL sentences. DeepASL achieves an average 94.5% word-level translation accuracy and an average 8.2% word error rate on translating unseen ASL sentences. Given its promising performance, we believe DeepASL represents a significant step towards breaking the communication barrier between deaf people and hearing majority, and thus has the significant potential to fundamentally change deaf people's lives."}),(0,o.jsx)(a.Z,{link:"./#/MobileHealth/MobilePhoneSensorCorrelatesofDepressiveSymptomSeverityinDailyLifeBehavior",imageURL:"./images/projects/DigitalMentalHealth.svg",title:"Digital Mental Health",content:"Depression is a common, burdensome, often recurring mental health disorder that frequently goes undetected and untreated. Mobile phones are ubiquitous and have an increasingly large complement of sensors that can potentially be useful in monitoring behavioral patterns that might be indicative of depressive symptoms. The objective of this work was to explore the detection of daily-life behavioral markers using mobile phone global positioning systems (GPS) and usage sensors, and their use in identifying depressive symptom severity. Our evaluation results show that features extracted from mobile phone sensor data, including GPS and phone usage, provided behavioral markers that were strongly related to depressive symptom severity. While these findings must be replicated in a larger study among participants with confirmed clinical symptoms, they suggest that phone sensors offer numerous clinical opportunities, including continuous monitoring of at-risk populations with little patient burden and interventions that can provide just-in-time outreach."})]})}},6822:function(){}}]);
//# sourceMappingURL=324.f8f5498f.chunk.js.map