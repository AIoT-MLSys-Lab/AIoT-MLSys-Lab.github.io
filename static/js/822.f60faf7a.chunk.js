"use strict";(self.webpackChunklabwebsite=self.webpackChunklabwebsite||[]).push([[822],{876:function(e,i,s){var t=s(8316),n=s(184),a={height:"300px",color:"#fff",lineHeight:"160px",textAlign:"center",background:"#364d79"};i.Z=function(){return(0,n.jsxs)(t.Z,{autoplay:!0,children:[(0,n.jsx)("div",{children:(0,n.jsx)("h3",{style:a,children:"1"})}),(0,n.jsx)("div",{children:(0,n.jsx)("h3",{style:a,children:"2"})}),(0,n.jsx)("div",{children:(0,n.jsx)("h3",{style:a,children:"3"})}),(0,n.jsx)("div",{children:(0,n.jsx)("h3",{style:a,children:"4"})})]})}},2649:function(e,i,s){s.r(i),s.d(i,{default:function(){return a}});s(2791),s(876);var t=s(4996),n=s(184);var a=function(){return(0,n.jsxs)("div",{className:"project_container",children:[(0,n.jsx)("div",{className:"project_title",children:"FlexDNN: Input-Adaptive On-Device Deep Learning for Efficient Mobile Vision"}),(0,n.jsx)("div",{className:"project_conference",children:"SEC'20"}),(0,n.jsx)("div",{className:"project_members",children:"Biyi Fang, Xiao Zeng, Faen Zhang, Hui Xu, and Mi Zhang."}),(0,n.jsxs)("div",{className:"project_buttons",children:[(0,n.jsxs)("a",{href:"https://mi-zhang.github.io/papers/2020_SEC_FlexDNN.pdf",className:"project_button",children:[(0,n.jsx)("svg",{t:"1687728426228",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"2399",width:"16",height:"16",children:(0,n.jsx)("path",{d:"M213.34016 0l597.34016 0q53.00224 0 90.50112 37.49888t37.49888 90.50112l0 768q0 53.00224-37.49888 90.50112t-90.50112 37.49888l-597.34016 0q-53.00224 0-90.50112-37.49888t-37.49888-90.50112l0-768q0-53.00224 37.49888-90.50112t90.50112-37.49888zM341.34016 725.34016l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM341.34016 554.65984l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM810.65984 85.34016l-597.34016 0q-17.67424 0-30.16704 12.4928t-12.4928 30.16704l0 768q0 17.67424 12.4928 30.16704t30.16704 12.4928l597.34016 0q17.67424 0 30.16704-12.4928t12.4928-30.16704l0-768q0-17.67424-12.4928-30.16704t-30.16704-12.4928zM341.34016 384l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM341.34016 213.34016l170.65984 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-170.65984 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928z",fill:"#ffffff","p-id":"2400"})}),"\xa0Paper"]}),(0,n.jsx)(t.Z,{inproceeding:"@inproceedings{flexdnn2020sec,",title:"title={{FlexDNN: Input-Adaptive On-Device Deep Learning for Efficient Mobile Vision}},",author:"author={Fang, Biyi and Zeng, Xiao and Zhang, Faen and Xu, Hui and Zhang, Mi},",booktitle:"booktitle={ACM/IEEE Symposium on Edge Computing (SEC)},",year:"year={2020}"})]}),(0,n.jsx)("div",{className:"project_collection",children:(0,n.jsx)("div",{children:(0,n.jsx)("img",{src:"./images/projects/FlexDNN_in.svg",alt:""})})}),(0,n.jsxs)("div",{className:"project_abstract",children:[(0,n.jsx)("div",{className:"project_abstract_title",children:"Abstract"}),(0,n.jsx)("div",{className:"project_abstract_content",children:"Mobile vision systems powered by the recent advancement in Deep Neural Networks (DNNs) are enabling a wide range of on-device video analytics applications. Considering mobile systems are constrained with limited resources, reducing resource demands of DNNs is crucial to realizing the full potential of these applications. In this paper, we present FlexDNN, an input-adaptive DNN-based framework for efficient on-device video analytics. To achieve this, FlexDNN takes the intrinsic dynamics of mobile videos into consideration, and dynamically adapts its model complexity to the difficulty levels of input video frames to achieve computation efficiency. FlexDNN addresses the key drawbacks of existing systems and pushes the state-of-theart forward. We use FlexDNN to build three representative ondevice video analytics applications, and evaluate its performance on both mobile CPU and GPU platforms. Our results show that FlexDNN significantly outperforms status quo approaches in accuracy, average CPU/GPU processing time per frame, frame drop rate, and energy consumption."})]})]})}},4996:function(e,i,s){s.d(i,{Z:function(){return l}});var t=s(9439),n=s(2791),a=s(9690),c=s(7309),r=s(184),l=function(e){var i=(0,n.useState)(!1),s=(0,t.Z)(i,2),l=s[0],o=s[1],d=function(){o(!1)};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)("button",{href:"",className:"project_button",onClick:function(){o(!0)},children:[(0,r.jsx)("svg",{t:"1687730915571",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"2419",width:"16",height:"16",children:(0,r.jsx)("path",{d:"M313.6 170.666667h469.333333c27.733333 0 42.666667 14.933333 42.666667 42.666666v512c0 8.533333-4.266667 17.066667-10.666667 23.466667l-36.266666 36.266667c-2.133333 2.133333-6.4 4.266667-6.4 2.133333-2.133333 0-2.133333-4.266667-2.133334-6.4v-539.733333c0-4.266667-2.133333-6.4-4.266666-10.666667s-6.4-4.266667-10.666667-4.266667h-398.933333c-8.533333 0-17.066667 4.266667-23.466667 10.666667l-36.266667 36.266667c-2.133333 2.133333-4.266667 6.4-2.133333 6.4 0 2.133333 4.266667 2.133333 6.4 2.133333h398.933333c4.266667 0 6.4 2.133333 10.666667 4.266667s4.266667 6.4 4.266667 10.666666v539.733334c0 4.266667-2.133333 6.4-4.266667 10.666666s-6.4 4.266667-10.666667 4.266667H213.333333c-4.266667 0-6.4-2.133333-10.666666-4.266667s-4.266667-6.4-4.266667-10.666666v-554.666667c0-8.533333 4.266667-17.066667 10.666667-23.466667l78.933333-78.933333c6.4-4.266667 14.933333-8.533333 25.6-8.533333z","p-id":"2420",fill:"#ffffff"})}),"\xa0BibTex"]}),(0,r.jsx)(a.Z,{title:"BibTex",open:l,onCancel:d,width:1e3,footer:[(0,r.jsx)(c.ZP,{onClick:d,children:"Return"},"back")],children:(0,r.jsxs)("div",{className:"popModalBG",children:[(0,r.jsx)("p",{children:e.inproceeding}),(0,r.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.title]}),(0,r.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.author]}),(0,r.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.booktitle]}),e.pages&&(0,r.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.pages]}),(0,r.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.year]}),e.numpages&&(0,r.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.numpages]}),e.address&&(0,r.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.address]}),e.location&&(0,r.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.location]}),(0,r.jsx)("p",{children:"}"})]})})]})}}}]);
//# sourceMappingURL=822.f60faf7a.chunk.js.map