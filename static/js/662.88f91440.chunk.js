"use strict";(self.webpackChunklabwebsite=self.webpackChunklabwebsite||[]).push([[662],{876:function(e,s,i){var n=i(8316),t=i(184),r={height:"300px",color:"#fff",lineHeight:"160px",textAlign:"center",background:"#364d79"};s.Z=function(){return(0,t.jsxs)(n.Z,{autoplay:!0,children:[(0,t.jsx)("div",{children:(0,t.jsx)("h3",{style:r,children:"1"})}),(0,t.jsx)("div",{children:(0,t.jsx)("h3",{style:r,children:"2"})}),(0,t.jsx)("div",{children:(0,t.jsx)("h3",{style:r,children:"3"})}),(0,t.jsx)("div",{children:(0,t.jsx)("h3",{style:r,children:"4"})})]})}},6662:function(e,s,i){i.r(s),i.d(s,{default:function(){return r}});i(2791),i(876);var n=i(4996),t=i(184);var r=function(){return(0,t.jsxs)("div",{className:"project_container",children:[(0,t.jsx)("div",{className:"project_title",children:"NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning for Continuous Mobile Vision"}),(0,t.jsx)("div",{className:"project_conference",children:"MobiCom'18"}),(0,t.jsx)("div",{className:"project_members",children:"Biyi Fang*, Xiao Zeng*, and Mi Zhang."}),(0,t.jsxs)("div",{className:"project_buttons",children:[(0,t.jsxs)("a",{href:"https://mi-zhang.github.io/papers/2018_MobiCom_NestDNN.pdf",className:"project_button",children:[(0,t.jsx)("svg",{t:"1687728426228",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"2399",width:"16",height:"16",children:(0,t.jsx)("path",{d:"M213.34016 0l597.34016 0q53.00224 0 90.50112 37.49888t37.49888 90.50112l0 768q0 53.00224-37.49888 90.50112t-90.50112 37.49888l-597.34016 0q-53.00224 0-90.50112-37.49888t-37.49888-90.50112l0-768q0-53.00224 37.49888-90.50112t90.50112-37.49888zM341.34016 725.34016l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM341.34016 554.65984l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM810.65984 85.34016l-597.34016 0q-17.67424 0-30.16704 12.4928t-12.4928 30.16704l0 768q0 17.67424 12.4928 30.16704t30.16704 12.4928l597.34016 0q17.67424 0 30.16704-12.4928t12.4928-30.16704l0-768q0-17.67424-12.4928-30.16704t-30.16704-12.4928zM341.34016 384l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM341.34016 213.34016l170.65984 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-170.65984 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928z",fill:"#ffffff","p-id":"2400"})}),"\xa0Paper"]}),(0,t.jsx)(n.Z,{inproceeding:"@inproceedings{fangzeng2018nestdnn,",title:"title = {{NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning for Continuous Mobile Vision}},",author:"author = {Fang, Biyi and Zeng, Xiao and Zhang, Mi},",booktitle:"booktitle = {Proceedings of the 24th Annual International Conference on Mobile Computing and Networking (MobiCom)},",year:"year = {2018},",pages:"pages = {115--127},",numpages:"numpages = {13},",address:"address = {New Delhi, India}"})]}),(0,t.jsx)("div",{className:"project_collection",children:(0,t.jsx)("div",{children:(0,t.jsx)("img",{src:"./images/projects/NestDNN_in.svg",alt:""})})}),(0,t.jsxs)("div",{className:"project_abstract",children:[(0,t.jsx)("div",{className:"project_abstract_title",children:"Abstract"}),(0,t.jsx)("div",{className:"project_abstract_content",children:"Mobile vision systems such as smartphones, drones, and augmented-reality headsets are revolutionizing our lives. These systems usually run multiple applications concurrently and their available resources at runtime are dynamic due to events such as starting new applications, closing existing applications, and application priority changes. In this paper, we present NestDNN, a framework that takes the dynamics of runtime resources into account to enable resourceaware multi-tenant on-device deep learning for mobile vision systems. NestDNN enables each deep learning model to offer flexible resource-accuracy trade-offs. At runtime, it dynamically selects the optimal resource-accuracy trade-off for each deep learning model to fit the model\u2019s resource demand to the system\u2019s available runtime resources. In doing so, NestDNN efficiently utilizes the limited resources in mobile vision systems to jointly maximize the performance of all the concurrently running applications. Our experiments show that compared to the resource-agnostic status quo approach, NestDNN achieves as much as 4.2% increase in inference accuracy, 2.0\xd7 increase in video frame processing rate and 1.7\xd7 reduction on energy consumption."})]})]})}},4996:function(e,s,i){i.d(s,{Z:function(){return o}});var n=i(9439),t=i(2791),r=i(9690),a=i(7309),c=i(184),o=function(e){var s=(0,t.useState)(!1),i=(0,n.Z)(s,2),o=i[0],l=i[1],d=function(){l(!1)};return(0,c.jsxs)(c.Fragment,{children:[(0,c.jsxs)("button",{href:"",className:"project_button",onClick:function(){l(!0)},children:[(0,c.jsx)("svg",{t:"1687730915571",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"2419",width:"16",height:"16",children:(0,c.jsx)("path",{d:"M313.6 170.666667h469.333333c27.733333 0 42.666667 14.933333 42.666667 42.666666v512c0 8.533333-4.266667 17.066667-10.666667 23.466667l-36.266666 36.266667c-2.133333 2.133333-6.4 4.266667-6.4 2.133333-2.133333 0-2.133333-4.266667-2.133334-6.4v-539.733333c0-4.266667-2.133333-6.4-4.266666-10.666667s-6.4-4.266667-10.666667-4.266667h-398.933333c-8.533333 0-17.066667 4.266667-23.466667 10.666667l-36.266667 36.266667c-2.133333 2.133333-4.266667 6.4-2.133333 6.4 0 2.133333 4.266667 2.133333 6.4 2.133333h398.933333c4.266667 0 6.4 2.133333 10.666667 4.266667s4.266667 6.4 4.266667 10.666666v539.733334c0 4.266667-2.133333 6.4-4.266667 10.666666s-6.4 4.266667-10.666667 4.266667H213.333333c-4.266667 0-6.4-2.133333-10.666666-4.266667s-4.266667-6.4-4.266667-10.666666v-554.666667c0-8.533333 4.266667-17.066667 10.666667-23.466667l78.933333-78.933333c6.4-4.266667 14.933333-8.533333 25.6-8.533333z","p-id":"2420",fill:"#ffffff"})}),"\xa0BibTex"]}),(0,c.jsx)(r.Z,{title:"BibTex",open:o,onCancel:d,width:1e3,footer:[(0,c.jsx)(a.ZP,{onClick:d,children:"Return"},"back")],children:(0,c.jsxs)("div",{className:"popModalBG",children:[(0,c.jsx)("p",{children:e.inproceeding}),(0,c.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.title]}),(0,c.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.author]}),(0,c.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.booktitle]}),e.pages&&(0,c.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.pages]}),(0,c.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.year]}),e.numpages&&(0,c.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.numpages]}),e.address&&(0,c.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.address]}),e.location&&(0,c.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.location]}),(0,c.jsx)("p",{children:"}"})]})})]})}}}]);
//# sourceMappingURL=662.88f91440.chunk.js.map