"use strict";(self.webpackChunklabwebsite=self.webpackChunklabwebsite||[]).push([[324],{4243:function(e,i,t){t.d(i,{Z:function(){return a}});t(2791);var n=t(184),a=function(e){var i=e.link,t=e.imageURL,a=e.title,s=e.content;return(0,n.jsx)("div",{children:(0,n.jsx)("a",{href:i,className:"projectContainer",children:(0,n.jsxs)("div",{className:"projectPanel",children:[(0,n.jsx)("div",{className:"projectPanelTitle",children:a}),(0,n.jsxs)("div",{className:"projectFlex",children:[(0,n.jsx)("div",{className:"projectPanelImgContainer",children:(0,n.jsx)("img",{src:t,alt:"",className:"projectPanelImg"})}),(0,n.jsx)("div",{className:"projectPanelTextContainer",children:(0,n.jsx)("div",{className:"projectPanelContent",children:(0,n.jsx)("div",{dangerouslySetInnerHTML:{__html:s}})})})]})]})})})}},6324:function(e,i,t){t.r(i);t(2791);var n=t(4676),a=(t(6822),t(4243)),s=t(184);i.default=function(){return(0,s.jsxs)("div",{children:[(0,s.jsx)(n.Z,{title:"Mobile Health",subtitle:"Research"}),(0,s.jsx)(a.Z,{link:"./#/MobileHealth/MobileDeepPill",imageURL:"./images/projects/EdgeAI.png",title:"MobileDeepPill: A Small-Footprint Mobile Deep Learning System for Recognizing Unconstrained Pill Images",content:"Correct identification of prescription pills based on their visual appearance is a key step required to assure patient safety and facilitate more effective patient care. With the availability of high-quality cameras and computational power on smartphones, it is possible and helpful to identify unknown prescription pills using smartphones. Towards this goal, in 2016, the U.S. National Library of Medicine (NLM) of the National Institutes of Health (NIH) announced a nationwide competition, calling for the creation of a mobile vision system that can recognize pills automatically from a mobile phone picture under unconstrained real-world settings. In this work, we present the design and evaluation of such mobile pill image recognition system called MobileDeepPill. The development of MobileDeepPill involves three key innovations: a triplet loss function which attains invariances to real-world noisiness that deteriorates the quality of pill images taken by mobile phones; a multi-CNNs model that collectively captures the shape, color and imprints characteristics of the pills; and a Knowledge Distillation-based deep model compression framework that significantly reduces the size of the multi-CNNs model without deteriorating its recognition performance. Our deep learning-based pill image recognition algorithm wins the First Prize (champion) of the NIH NLM Pill Image Recognition Challenge. Given its promising performance, we believe MobileDeepPill helps NIH tackle a critical problem with significant societal impact and will benefit millions of healthcare personnel and the general public."}),(0,s.jsx)(a.Z,{link:"./#/MobileHealth/SharpEar",imageURL:"./images/projects/EdgeAI.png",title:"SharpEar: Real-Time Speech Enhancement in Noisy Environments ",content:"Wi-Fi imaging has attracted significant interests due to the ubiquitous availability of Wi-Fi devices today. In this work, we present Wi-Fi See It All (WiSIA), a versatile Wi-Fi imaging system built upon commercial off-the-shelf (COTS) Wi-Fi devices, which is able to simultaneously detect objects and humans, segment their boundaries, and identify them within the image plane. To achieve this, WiSIA utilizes three techniques. First, instead of constructing the image plane at the receiver side using a high-cost antenna array and complex parameter estimation, WiSIA pushes the image plane to the object side with two pairs of transceivers and 2D-IFFT. Second, WiSIA extracts the specific physical signature of the signals reflected from multiple objects to segment their boundaries. Third, WiSIA incorporates a cGAN (conditional Generative Adversarial Network) to enhance the boundary of different objects. We have implemented WiSIA using COTS Wi-Fi devices and evaluated it using a rich set of experiments. Our results demonstrate the efficacy of WiSIA. It outperforms the state-of-the-art vision-based method in dark and occlusion scenarios, demonstrating its superiority in such challenge scenarios."}),(0,s.jsx)(a.Z,{link:"./#/MobileHealth/DeepASL",imageURL:"./images/projects/EdgeAI.png",title:"DeepASL: Enabling Ubiquitous and Non-Intrusive Word and Sentence-Level Sign Language Translation",content:"There is an undeniable communication barrier between deaf people and people with normal hearing ability. Although innovations in sign language translation technology aim to tear down this communication barrier, the majority of existing sign language translation systems are either intrusive or constrained by resolution or ambient lighting conditions. Moreover, these existing systems can only perform single-sign ASL translation rather than sentence-level translation, making them much less useful in daily-life communication scenarios. In this work, we fill this critical gap by presenting DeepASL, a transformative deep learning-based sign language translation technology that enables ubiquitous and non-intrusive American Sign Language (ASL) translation at both word and sentence levels. DeepASL uses infrared light as its sensing mechanism to non-intrusively capture the ASL signs. It incorporates a novel hierarchical bidirectional deep recurrent neural network (HB-RNN) and a probabilistic framework based on Connectionist Temporal Classification (CTC) for word-level and sentence-level ASL translation respectively. To evaluate its performance, we have collected 7, 306 samples from 11 participants, covering 56 commonly used ASL words and 100 ASL sentences. DeepASL achieves an average 94.5% word-level translation accuracy and an average 8.2% word error rate on translating unseen ASL sentences. Given its promising performance, we believe DeepASL represents a significant step towards breaking the communication barrier between deaf people and hearing majority, and thus has the significant potential to fundamentally change deaf people's lives."}),(0,s.jsx)(a.Z,{link:"./#/MobileHealth/MobilePhoneSensorCorrelatesofDepressiveSymptomSeverityinDailyLifeBehavior",imageURL:"./images/projects/EdgeAI.png",title:"Mobile Phone Sensor Correlates of Depressive Symptom Severity in Daily-Life Behavior: An Exploratory Study",content:"Depression is a common, burdensome, often recurring mental health disorder that frequently goes undetected and untreated. Mobile phones are ubiquitous and have an increasingly large complement of sensors that can potentially be useful in monitoring behavioral patterns that might be indicative of depressive symptoms. The objective of this work was to explore the detection of daily-life behavioral markers using mobile phone global positioning systems (GPS) and usage sensors, and their use in identifying depressive symptom severity. Our evaluation results show that features extracted from mobile phone sensor data, including GPS and phone usage, provided behavioral markers that were strongly related to depressive symptom severity. While these findings must be replicated in a larger study among participants with confirmed clinical symptoms, they suggest that phone sensors offer numerous clinical opportunities, including continuous monitoring of at-risk populations with little patient burden and interventions that can provide just-in-time outreach."})]})}},6822:function(){}}]);
//# sourceMappingURL=324.0813c434.chunk.js.map