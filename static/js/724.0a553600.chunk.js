"use strict";(self.webpackChunklabwebsite=self.webpackChunklabwebsite||[]).push([[724],{4243:function(e,i,t){t.d(i,{Z:function(){return n}});t(2791);var a=t(184),n=function(e){var i=e.link,t=e.imageURL,n=e.title,s=e.content;return(0,a.jsx)("div",{children:(0,a.jsx)("a",{href:i,className:"projectContainer",children:(0,a.jsxs)("div",{className:"projectPanel",children:[(0,a.jsx)("div",{className:"projectPanelTitle",children:n}),(0,a.jsxs)("div",{className:"projectFlex",children:[(0,a.jsx)("div",{className:"projectPanelImgContainer",children:(0,a.jsx)("img",{src:t,alt:"",className:"projectPanelImg"})}),(0,a.jsx)("div",{className:"projectPanelTextContainer",children:(0,a.jsx)("div",{className:"projectPanelContent",children:(0,a.jsx)("div",{dangerouslySetInnerHTML:{__html:s}})})})]})]})})})}},1724:function(e,i,t){t.r(i);t(2791);var a=t(4676),n=(t(6822),t(4243)),s=t(184);i.default=function(){return(0,s.jsxs)("div",{children:[(0,s.jsx)(a.Z,{title:"Large Language Models and Generative AI",subtitle:"Research"}),(0,s.jsx)(n.Z,{link:"./#/LLM/EfficientLLM",imageURL:"./images/projects/EfficientLLM.svg",title:"Efficient Large Language Models: A Survey",content:"Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of the research developments in efficient LLMs and inspire them to contribute to this important and exciting field."}),(0,s.jsx)(n.Z,{link:"./#/LLM/SVDLLM",imageURL:"./images/projects/SVM.png",title:"SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",content:"The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weight after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation under high compression ratios. We evaluate SVD-LLM on a total of 10 datasets and eight models from three different LLM families at four different scales. Our results demonstrate the superiority of SVD-LLM over state-of-the-arts, especially at high model compression ratios."}),(0,s.jsx)(n.Z,{link:"./#/LLM/MEIT",imageURL:"./images/projects/MEIT.png",title:"MEIT: Multi-Modal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation",content:"Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for monitoring cardiac conditions and is crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is time-consuming and requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the first attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open-source LLMs using more than 800,000 ECG reports. MEIT's results underscore the superior performance of instruction-tuned LLMs, showcasing their proficiency in quality report generation, zero-shot capabilities, and resilience to signal perturbation. These findings emphasize the efficacy of our MEIT framework and its potential for real-world clinical application."}),(0,s.jsx)(n.Z,{link:"./#/LLM/D2O",imageURL:"./images/projects/D2O.svg",title:"D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models",content:"Efficient inference in Large Language Models (LLMs) is impeded by the growing memory demands of key-value (KV) caching, especially for longer sequences. Traditional KV cache eviction strategies, which prioritize less critical KV-pairs based on attention scores, often degrade generation quality, leading to issues such as context loss or hallucinations. To address this, we introduce Dynamic Discriminative Operations (D2O), a novel method that utilizes two-level discriminative strategies to optimize KV cache size without fine-tuning, while preserving essential context. Initially, by observing varying densities of attention weights between shallow and deep layers, we use this insight to determine which layers should avoid excessive eviction to minimize information loss. Subsequently, for the eviction strategy in each layer, D2O innovatively incorporates a compensation mechanism that maintains a similarity threshold to re-discriminate the importance of previously discarded tokens, determining whether they should be recalled and merged with similar tokens. Our approach not only achieves significant memory savings and enhances inference throughput by more than 3 times but also maintains high-quality long-text generation. Extensive experiments across various benchmarks and LLM architectures have demonstrated that D2O significantly enhances performance with a constrained KV cache budget."}),(0,s.jsx)(n.Z,{link:"./#/LLM/ETP",imageURL:"./images/projects/ETP.svg",title:"ETP: Learning Transferable ECG Representations via ECG-Text Pre-Training",content:"In the domain of cardiovascular healthcare, the Electrocardiogram (ECG) serves as a critical, non-invasive diagnostic tool. Although recent strides in self-supervised learning (SSL) have been promising for ECG representation learning, these techniques often require annotated samples and struggle with classes not present in the fine-tuning stages. To address these limitations, we introduce ECG-Text Pre-training (ETP), an innovative framework designed to learn cross-modal representations that link ECG signals with textual reports. For the first time, this framework leverages the zero-shot classification task in the ECG domain. ETP employs an ECG encoder along with a pre-trained language model to align ECG signals with their corresponding textual reports. The proposed framework excels in both linear evaluation and zero-shot classification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets, showcasing its ability for robust and generalizable cross-modal ECG feature learning."}),(0,s.jsx)(n.Z,{link:"./#/LLM/FambaV",imageURL:"./images/projects/FambaV.png",title:"Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion",content:"Mamba and Vision Mamba (Vim) models have shown their potential as an alternative to methods based on Transformer architecture. This work introduces Fast Mamba for Vision (Famba-V), a cross-layer token fusion technique to enhance the training efficiency of Vim models. The key idea of Famba-V is to identify and fuse similar tokens across different Vim layers based on a suit of cross-layer strategies instead of simply applying token fusion uniformly across all the layers that existing works propose. We evaluate the performance of Famba-V on CIFAR-100. Our results show that Famba-V is able to enhance the training efficiency of Vim models by reducing both training time and peak memory usage during training. Moreover, the proposed cross-layer strategies allow Famba-V to deliver superior accuracy-efficiency trade-offs. These results all together demonstrate Famba-V as a promising efficiency enhancement technique for Vim models."})]})}},6822:function(){}}]);
//# sourceMappingURL=724.0a553600.chunk.js.map