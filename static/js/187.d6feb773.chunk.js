"use strict";(self.webpackChunklabwebsite=self.webpackChunklabwebsite||[]).push([[187],{876:function(e,t,s){var r=s(8316),i=s(184),c={height:"300px",color:"#fff",lineHeight:"160px",textAlign:"center",background:"#364d79"};t.Z=function(){return(0,i.jsxs)(r.Z,{autoplay:!0,children:[(0,i.jsx)("div",{children:(0,i.jsx)("h3",{style:c,children:"1"})}),(0,i.jsx)("div",{children:(0,i.jsx)("h3",{style:c,children:"2"})}),(0,i.jsx)("div",{children:(0,i.jsx)("h3",{style:c,children:"3"})}),(0,i.jsx)("div",{children:(0,i.jsx)("h3",{style:c,children:"4"})})]})}},6187:function(e,t,s){s.r(t),s.d(t,{default:function(){return c}});s(2791),s(876);var r=s(4996),i=s(184);var c=function(){return(0,i.jsxs)("div",{className:"project_container",children:[(0,i.jsx)("div",{className:"project_title",children:"Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?"}),(0,i.jsx)("div",{className:"project_conference",children:"NeurIPS'20"}),(0,i.jsx)("div",{className:"project_members",children:"Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, and Mi Zhang."}),(0,i.jsxs)("div",{className:"project_buttons",children:[(0,i.jsxs)("a",{href:"https://arxiv.org/pdf/2006.06936.pdf",className:"project_button",children:[(0,i.jsx)("svg",{t:"1687728426228",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"2399",width:"16",height:"16",children:(0,i.jsx)("path",{d:"M213.34016 0l597.34016 0q53.00224 0 90.50112 37.49888t37.49888 90.50112l0 768q0 53.00224-37.49888 90.50112t-90.50112 37.49888l-597.34016 0q-53.00224 0-90.50112-37.49888t-37.49888-90.50112l0-768q0-53.00224 37.49888-90.50112t90.50112-37.49888zM341.34016 725.34016l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM341.34016 554.65984l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM810.65984 85.34016l-597.34016 0q-17.67424 0-30.16704 12.4928t-12.4928 30.16704l0 768q0 17.67424 12.4928 30.16704t30.16704 12.4928l597.34016 0q17.67424 0 30.16704-12.4928t12.4928-30.16704l0-768q0-17.67424-12.4928-30.16704t-30.16704-12.4928zM341.34016 384l341.34016 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-341.34016 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928zM341.34016 213.34016l170.65984 0q17.67424 0 30.16704 12.4928t12.4928 30.16704-12.4928 30.16704-30.16704 12.4928l-170.65984 0q-17.67424 0-30.16704-12.4928t-12.4928-30.16704 12.4928-30.16704 30.16704-12.4928z",fill:"#ffffff","p-id":"2400"})}),"\xa0Paper"]}),(0,i.jsx)(r.Z,{inproceeding:"@inproceedings{arch2vec2020neurips,",title:"title={{Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?}},",author:"author={Yan, Shen and Zheng, Yu and Ao, Wei and Zeng, Xiao and Zhang, Mi},",booktitle:"booktitle={Conference on Neural Information Processing Systems (NeurIPS)},",year:"year={2020}"}),(0,i.jsxs)("a",{href:"https://github.com/MSU-MLSys-Lab/arch2vec",className:"project_button",children:[(0,i.jsxs)("svg",{t:"1687728592166",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"3643",width:"16",height:"16",children:[(0,i.jsx)("path",{d:"M438.4 849.1l222.7-646.7c0.2-0.5 0.3-1.1 0.4-1.6L438.4 849.1z",opacity:".224","p-id":"3644",fill:"#ffffff"}),(0,i.jsx)("path",{d:"M661.2 168.7h-67.5c-3.4 0-6.5 2.2-7.6 5.4L354.7 846c-0.3 0.8-0.4 1.7-0.4 2.6 0 4.4 3.6 8 8 8h67.8c3.4 0 6.5-2.2 7.6-5.4l0.7-2.1 223.1-648.3 7.4-21.4c0.3-0.8 0.4-1.7 0.4-2.6-0.1-4.5-3.6-8.1-8.1-8.1zM954.6 502.1c-0.8-1-1.7-1.9-2.7-2.7l-219-171.3c-3.5-2.7-8.5-2.1-11.2 1.4-1.1 1.4-1.7 3.1-1.7 4.9v81.3c0 2.5 1.1 4.8 3.1 6.3l115 90-115 90c-1.9 1.5-3.1 3.8-3.1 6.3v81.3c0 4.4 3.6 8 8 8 1.8 0 3.5-0.6 4.9-1.7l219-171.3c6.9-5.4 8.2-15.5 2.7-22.5zM291.1 328.1l-219 171.3c-1 0.8-1.9 1.7-2.7 2.7-5.4 7-4.2 17 2.7 22.5l219 171.3c1.4 1.1 3.1 1.7 4.9 1.7 4.4 0 8-3.6 8-8v-81.3c0-2.5-1.1-4.8-3.1-6.3l-115-90 115-90c1.9-1.5 3.1-3.8 3.1-6.3v-81.3c0-1.8-0.6-3.5-1.7-4.9-2.7-3.5-7.7-4.1-11.2-1.4z","p-id":"3645",fill:"#ffffff"})]}),"\xa0Code"]}),(0,i.jsxs)("a",{href:"https://mi-zhang.github.io/papers/2020_NeurIPS_arch2vec_poster.pdf",className:"project_button",children:[(0,i.jsxs)("svg",{t:"1689885276515",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"1916",width:"16",height:"16",children:[(0,i.jsx)("path",{d:"M884.4 810.5l-745.8 0.3 0.1-595.9 745.8-0.2 0.3 595.8h-0.4m0-670H138.7c-41.2-0.1-74.6 33.2-74.7 74.2v596.1c0.1 41.1 33.6 74.3 74.7 74.2h745.7c41.1 0 74.5-33.1 74.7-74.2V214.7c-0.1-41.1-33.5-74.3-74.7-74.2","p-id":"1917",fill:"#ffffff"}),(0,i.jsx)("path",{d:"M250.5 587.2h149.2c20.6 0 37.3-16.7 37.3-37.2 0-20.6-16.7-37.2-37.3-37.2H250.5c-20.6 0-37.3 16.7-37.3 37.2 0 20.5 16.7 37.2 37.3 37.2m298.4 74.4H250.5c-20.6 0-37.3 16.7-37.3 37.2s16.7 37.2 37.3 37.2h298.4c20.6 0 37.3-16.7 37.3-37.2s-16.7-37.2-37.3-37.2m-37.3-372.2v297.8H810V289.4H511.6z m223.7 223.3H586.2V363.8h149.2v148.9z m0 0","p-id":"1918",fill:"#ffffff"})]}),"\xa0Poster"]})]}),(0,i.jsx)("div",{className:"project_collection",children:(0,i.jsx)("div",{children:(0,i.jsx)("img",{src:"./images/projects/arch2ve_in.svg",alt:"",className:"project_collection_img"})})}),(0,i.jsxs)("div",{className:"project_abstract",children:[(0,i.jsx)("div",{className:"project_abstract_title",children:"Abstract"}),(0,i.jsx)("div",{className:"project_abstract_content",children:"Existing Neural Architecture Search (NAS) methods either encode neural architectures using discrete encodings that do not scale well, or adopt supervised learning-based methods to jointly learn architecture representations and optimize architecture search on such representations which incurs search bias. Despite the widespread use, architecture representations learned in NAS are still poorly understood. We observe that the structural properties of neural architectures are hard to preserve in the latent space if architecture representation learning and search are coupled, resulting in less effective search performance. In this work, we find empirically that pre-training architecture representations using only neural architectures without their accuracies as labels improves the downstream architecture search efficiency. To explain this finding, we visualize how unsupervised architecture representation learning better encourages neural architectures with similar connections and operators to cluster together. This helps map neural architectures with similar performance to the same regions in the latent space and makes the transition of architectures in the latent space relatively smooth, which considerably benefits diverse downstream search strategies."})]})]})}},4996:function(e,t,s){s.d(t,{Z:function(){return l}});var r=s(9439),i=s(2791),c=s(9690),n=s(7309),a=s(184),l=function(e){var t=(0,i.useState)(!1),s=(0,r.Z)(t,2),l=s[0],h=s[1],o=function(){h(!1)};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)("button",{href:"",className:"project_button",onClick:function(){h(!0)},children:[(0,a.jsx)("svg",{t:"1687730915571",class:"icon",viewBox:"0 0 1024 1024",version:"1.1",xmlns:"http://www.w3.org/2000/svg","p-id":"2419",width:"16",height:"16",children:(0,a.jsx)("path",{d:"M313.6 170.666667h469.333333c27.733333 0 42.666667 14.933333 42.666667 42.666666v512c0 8.533333-4.266667 17.066667-10.666667 23.466667l-36.266666 36.266667c-2.133333 2.133333-6.4 4.266667-6.4 2.133333-2.133333 0-2.133333-4.266667-2.133334-6.4v-539.733333c0-4.266667-2.133333-6.4-4.266666-10.666667s-6.4-4.266667-10.666667-4.266667h-398.933333c-8.533333 0-17.066667 4.266667-23.466667 10.666667l-36.266667 36.266667c-2.133333 2.133333-4.266667 6.4-2.133333 6.4 0 2.133333 4.266667 2.133333 6.4 2.133333h398.933333c4.266667 0 6.4 2.133333 10.666667 4.266667s4.266667 6.4 4.266667 10.666666v539.733334c0 4.266667-2.133333 6.4-4.266667 10.666666s-6.4 4.266667-10.666667 4.266667H213.333333c-4.266667 0-6.4-2.133333-10.666666-4.266667s-4.266667-6.4-4.266667-10.666666v-554.666667c0-8.533333 4.266667-17.066667 10.666667-23.466667l78.933333-78.933333c6.4-4.266667 14.933333-8.533333 25.6-8.533333z","p-id":"2420",fill:"#ffffff"})}),"\xa0BibTex"]}),(0,a.jsx)(c.Z,{title:"BibTex",open:l,onCancel:o,width:1e3,footer:[(0,a.jsx)(n.ZP,{onClick:o,children:"Return"},"back")],children:(0,a.jsxs)("div",{className:"popModalBG",children:[(0,a.jsx)("p",{children:e.inproceeding}),(0,a.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.title]}),(0,a.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.author]}),(0,a.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.booktitle]}),e.pages&&(0,a.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.pages]}),(0,a.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.year]}),e.numpages&&(0,a.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.numpages]}),e.address&&(0,a.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.address]}),e.location&&(0,a.jsxs)("p",{children:["\xa0\xa0\xa0\xa0",e.location]}),(0,a.jsx)("p",{children:"}"})]})})]})}}}]);
//# sourceMappingURL=187.d6feb773.chunk.js.map